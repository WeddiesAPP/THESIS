{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "831cdc19-90d8-4840-bf5a-e24631be601b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/dbrink/.local/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: transformers in /home/dbrink/.local/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: bitsandbytes in /home/dbrink/.local/lib/python3.10/site-packages (0.45.5)\n",
      "Requirement already satisfied: peft in /home/dbrink/.local/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: torch in /home/dbrink/.local/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: Pillow in /home/dbrink/.local/lib/python3.10/site-packages (11.2.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dbrink/.local/lib/python3.10/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/dbrink/.local/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/dbrink/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/dbrink/.local/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/dbrink/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/dbrink/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/dbrink/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/dbrink/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/dbrink/.local/lib/python3.10/site-packages (from datasets) (0.31.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/dbrink/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dbrink/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/dbrink/.local/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/dbrink/.local/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/dbrink/.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/dbrink/.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/dbrink/.local/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/dbrink/.local/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/dbrink/.local/lib/python3.10/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/dbrink/.local/lib/python3.10/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/dbrink/.local/lib/python3.10/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/dbrink/.local/lib/python3.10/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/dbrink/.local/lib/python3.10/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/dbrink/.local/lib/python3.10/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/dbrink/.local/lib/python3.10/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/dbrink/.local/lib/python3.10/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/dbrink/.local/lib/python3.10/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/dbrink/.local/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/dbrink/.local/lib/python3.10/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/dbrink/.local/lib/python3.10/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/dbrink/.local/lib/python3.10/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.3.0->torch) (59.6.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/dbrink/.local/lib/python3.10/site-packages (from peft) (1.7.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/dbrink/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/dbrink/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/dbrink/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/dbrink/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/dbrink/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/dbrink/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/dbrink/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2020.6.20)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/dbrink/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/dbrink/.local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers bitsandbytes peft torch Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c08b17-4609-407e-901a-5dadac7336ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbrink/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x500 at 0x7F8728488BB0>, 'captions': {'action': [\"she is holding a horse's reins.\", 'she is showing a horse.', 'she is holding the horse'], 'object': ['A female jockey leading a white horse in front of an audience.', 'A woman is holding a horse on a leash', \"A white horse at a horse show with it's trainer.\", 'Woman in a competition signalling horse to stand still', 'A woman training a white horse in a contest.'], 'rationale': ['she is competing in a horse show.', 'she wants to win.', 'to train with him'], 'scene': ['at a horse show.', 'it is a horse show.', 'at a training site']}, 'confidence': {'action': [5.0, 5.0, 5.0], 'rationale': [5.0, 5.0, 4.0], 'scene': [5.0, 5.0, 4.0]}, 'purity': {'action': [-0.583791196346283, -1.024692177772522, -0.5447486639022827], 'rationale': [-1.0663983821868896, -1.0827453136444092, -1.575655221939087], 'scene': [-0.9763333797454834, -0.8623560667037964, -1.1825969219207764]}, 'diversity': {'action': 22.134903354056974, 'rationale': 4.599683102760639, 'scene': 51.87720847537742}, 'modified_captions': {'action': [\"She is holding a horse's reins.\", 'She is showing a horse.', 'She is holding the horse.'], 'rationale': ['She is competing in a horse show.', 'She wants to win.', 'It is reasonable to believe that she is training with him.'], 'scene': ['At a horse show.', 'It is a horse show.', 'It seems to be at a training site.']}}\n",
      "Total samples in test set: 13498\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load only the 'test' split\n",
    "dataset = load_from_disk(\"modified_hl_final\")\n",
    "\n",
    "# Check a few samples\n",
    "print(dataset[0])\n",
    "print(f\"Total samples in test set: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1d650-5240-4ce9-bf1b-65e3250b53c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45627e2-e09f-4d80-a63c-c4f5efe29982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 11:57:33.415068: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-24 11:57:33.427210: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748080653.442007 3172877 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748080653.446457 3172877 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748080653.457561 3172877 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748080653.457579 3172877 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748080653.457585 3172877 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748080653.457590 3172877 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-24 11:57:33.461733: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,242,880 || all params: 3,750,004,736 || trainable%: 0.1398\n",
      "{5: 75309, 4: 32592, 2: 2219, 3: 10552, 1: 799}\n",
      "\n",
      " Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: 1.5043\n",
      "\n",
      " Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Batch 1561:  21%|██        | 1561/7592 [16:34<1:06:52,  1.50it/s, loss=1.62] "
     ]
    }
   ],
   "source": [
    "#Reference: https://github.com/huggingface/notebooks/blob/main/peft/Fine_tune_BLIP2_on_an_image_captioning_dataset_PEFT.ipynb\n",
    "from datasets import load_from_disk\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "prompt_map = {\n",
    "    \"scene\": \"Where is the picture taken?\",\n",
    "    \"action\": \"What is the subject doing?\",\n",
    "    \"rationale\": \"Why is the subject doing it?\"\n",
    "}\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "model_id = \"Salesforce/blip2-opt-2.7b\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Quantization config (8-bit)\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "# Load processor and model\n",
    "processor = Blip2Processor.from_pretrained(model_id)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Freeze vision + Q-Former\n",
    "model.vision_model.requires_grad_(False)\n",
    "model.qformer.requires_grad_(False)\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset\n",
    "# -----------------------------\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.samples = []\n",
    "        self.processor = processor\n",
    "\n",
    "        for example in dataset:\n",
    "            image = example[\"image\"]\n",
    "            for axis in [\"scene\", \"action\", \"rationale\"]:\n",
    "                prompt = prompt_map[axis]\n",
    "                for caption, confidence in zip(\n",
    "                    example[\"modified_captions\"][axis],\n",
    "                    example[\"confidence\"][axis]\n",
    "                ):\n",
    "                    self.samples.append({\n",
    "                        \"image\": image,\n",
    "                        \"prompt\": prompt,\n",
    "                        \"caption\": caption,\n",
    "                        \"confidence\": confidence,  \n",
    "                        \"bin\": int(confidence),    # 1-5 \n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        encoding = self.processor(images=sample[\"image\"], return_tensors=\"pt\")\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "        prompt = sample[\"prompt\"]\n",
    "        caption = sample[\"caption\"]\n",
    "\n",
    "        prompt_ids = self.processor.tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "        caption_ids = self.processor.tokenizer(\n",
    "            \" \" + caption + self.processor.tokenizer.eos_token, add_special_tokens=False\n",
    "        )[\"input_ids\"]\n",
    "        input_ids = prompt_ids + caption_ids\n",
    "        labels = [-100] * len(prompt_ids) + caption_ids\n",
    "\n",
    "        encoding[\"input_ids\"] = torch.tensor(input_ids)\n",
    "        encoding[\"labels\"] = torch.tensor(labels)\n",
    "        return encoding\n",
    "\n",
    "    def get_bins(self):\n",
    "        from collections import defaultdict\n",
    "        bins = defaultdict(list)\n",
    "        for idx, s in enumerate(self.samples):\n",
    "            bins[s[\"bin\"]].append(idx)\n",
    "        return bins\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Pad input_ids and labels to the max length in batch\n",
    "    input_ids = [example[\"input_ids\"] for example in batch]\n",
    "    labels = [example[\"labels\"] for example in batch]\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in batch])\n",
    "\n",
    "    # Padding\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=processor.tokenizer.pad_token_id\n",
    "    )\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        labels, batch_first=True, padding_value=-100\n",
    "    )\n",
    "\n",
    "    # Attention mask from input_ids\n",
    "    attention_mask = (input_ids != processor.tokenizer.pad_token_id).long()\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"pixel_values\": pixel_values,\n",
    "    }\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "import csv\n",
    "avg_losses = []\n",
    "batch_losses = [] \n",
    "hf_dataset = load_from_disk(\"modified_hl_final\")\n",
    "train_dataset = ImageCaptioningDataset(hf_dataset, processor)\n",
    "\n",
    "bins = train_dataset.get_bins()\n",
    "bin_sizes = {k: len(v) for k, v in bins.items()}\n",
    "print(bin_sizes)\n",
    "# Sample-weights:  1/size of bin\n",
    "weights = []\n",
    "for s in train_dataset.samples:\n",
    "    b = s[\"bin\"]\n",
    "    weights.append(1.0 / bin_sizes[b])\n",
    "weights = torch.DoubleTensor(weights)\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(train_dataset), replacement=True)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    sampler=sampler,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "model.train()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(5): \n",
    "    print(f\"\\n Epoch {epoch + 1}\")\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\", leave=False)\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for idx, batch in enumerate(progress_bar):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        pixel_values = batch[\"pixel_values\"].to(device, torch.float16)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "            labels=labels \n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        global_batch_idx = epoch * len(train_dataloader) + idx \n",
    "        batch_losses.append((global_batch_idx, loss.item()))\n",
    "        epoch_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update tqdm bar\n",
    "        progress_bar.set_description(f\"Epoch {epoch + 1} | Batch {idx + 1}\")\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    avg_losses.append((epoch + 1, avg_loss))\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} average loss: {avg_loss:.4f}\")\n",
    "    model.save_pretrained(f\"SAMPLINGEOSLABELblip2-finetuned-full-epoch{epoch+1}\")\n",
    "    processor.save_pretrained(f\"SAMPLINGEOSLABELblip2-finetuned-full-epoch{epoch+1}\")\n",
    "with open(\"SAMPLINGEOSLABELaverage_epoch_losses.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Epoch\", \"Average Loss\"])\n",
    "    writer.writerows(avg_losses)\n",
    "\n",
    "print(\"Saved average losses to average_epoch_losses.csv\")\n",
    "\n",
    "with open(\"SAMPLINGEOSLABELbatch_losses.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Batch\", \"Loss\"])\n",
    "    writer.writerows(batch_losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6e22c9-373f-46ad-979a-0013ce79ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_captions = 0\n",
    "for example in dataset:\n",
    "    for axis in [\"scene\", \"action\", \"rationale\"]:\n",
    "        total_captions += len(example[\"modified_captions\"].get(axis, []))\n",
    "print(\"Total captions:\", total_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c0a21-13cf-4765-bdee-f03ab4cdfbdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3b9040-b280-42ea-a4eb-14efbb979fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
